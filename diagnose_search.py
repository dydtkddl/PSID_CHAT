#!/usr/bin/env python3
"""
ì»´í“¨í„°ê³µí•™ê³¼ ê²€ìƒ‰ í’ˆì§ˆ ì§„ë‹¨ ìŠ¤í¬ë¦½íŠ¸
ì‹¤ì œ FAISSì—ì„œ ì–´ë–¤ ë¬¸ì„œê°€ ê²€ìƒ‰ë˜ëŠ”ì§€ í™•ì¸
"""

import os
import sys
from pathlib import Path

# API í‚¤ ë¡œë“œ
try:
    import tomllib
except ImportError:
    import tomli as tomllib

secrets_path = Path(__file__).parent / ".streamlit" / "secrets.toml"
if secrets_path.exists():
    with open(secrets_path, "rb") as f:
        secrets = tomllib.load(f)
    os.environ.setdefault("OPENAI_API_KEY", secrets.get("OPENAI_API_KEY", ""))

from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from chains import get_multi_year_vector_store, get_retriever_chain
from query_parser import parse_query


def diagnose_search(query: str, category: str = "undergrad_rules", cohort: str = "2025"):
    """íŠ¹ì • ì¿¼ë¦¬ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ ì§„ë‹¨"""
    print("=" * 80)
    print(f"ğŸ” ì§„ë‹¨: '{query}'")
    print(f"   ì¹´í…Œê³ ë¦¬: {category}, ì½”í˜¸íŠ¸: {cohort}")
    print("=" * 80)
    
    # 1. ì¿¼ë¦¬ íŒŒì‹± ê²°ê³¼
    print("\n[1] Query Parser ê²°ê³¼:")
    meta_filter, hints = parse_query(query)
    print(f"  Meta Filter: {meta_filter}")
    print(f"  Routing Hints: {hints}")
    
    # 2. Vector Store ë¡œë“œ
    print(f"\n[2] Vector Store ë¡œë“œ ì¤‘...")
    try:
        vs = get_multi_year_vector_store(category, cohort)
        print(f"  âœ… ë¡œë“œ ì„±ê³µ! ì´ ë²¡í„° ìˆ˜: {vs.index.ntotal}")
    except Exception as e:
        print(f"  âŒ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # 3. ì§ì ‘ ë²¡í„° ê²€ìƒ‰ (ì‹œë§¨í‹± ì„œì¹˜)
    print(f"\n[3] ë²¡í„° ê²€ìƒ‰ (Semantic Search) - Top 10:")
    semantic_docs = vs.similarity_search(query, k=10)
    for i, doc in enumerate(semantic_docs, 1):
        meta = doc.metadata
        dept = extract_dept_from_content(doc.page_content)
        year = meta.get("_cohort_year", "?")
        source = meta.get("sourceFile", meta.get("source", "?"))
        preview = doc.page_content[:150].replace("\n", " ")
        
        print(f"\n  [{i}] ì—°ë„:{year} | í•™ê³¼:{dept} | ì¶œì²˜:{source}")
        print(f"      ë‚´ìš©: {preview}...")
    
    # 4. Retriever Chain ê²€ìƒ‰ (Hybrid)
    print(f"\n[4] Retriever Chain (Hybrid) - Top 5:")
    retriever = get_retriever_chain(vs, meta_filter=meta_filter, top_k=5, primary_cohort=cohort)
    hybrid_docs = retriever.invoke(query)
    for i, doc in enumerate(hybrid_docs, 1):
        meta = doc.metadata
        dept = extract_dept_from_content(doc.page_content)
        year = meta.get("_cohort_year", "?")
        source = meta.get("sourceFile", meta.get("source", "?"))
        preview = doc.page_content[:150].replace("\n", " ")
        
        print(f"\n  [{i}] ì—°ë„:{year} | í•™ê³¼:{dept} | ì¶œì²˜:{source}")
        print(f"      ë‚´ìš©: {preview}...")
    
    # 5. í‚¤ì›Œë“œ ì§ì ‘ ê²€ìƒ‰
    print(f"\n[5] í‚¤ì›Œë“œ ì§ì ‘ ê²€ìƒ‰ ('ì»´í“¨í„°ê³µí•™' í¬í•¨ ë¬¸ì„œ):")
    keyword_matches = []
    try:
        for doc_id, doc in vs.docstore._dict.items():
            if "ì»´í“¨í„°ê³µí•™" in doc.page_content:
                keyword_matches.append(doc)
                if len(keyword_matches) >= 10:
                    break
    except Exception as e:
        print(f"  âŒ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        keyword_matches = []
    
    print(f"  ì´ {len(keyword_matches)}ê°œ ë°œê²¬")
    for i, doc in enumerate(keyword_matches[:5], 1):
        meta = doc.metadata
        year = meta.get("_cohort_year", "?")
        source = meta.get("sourceFile", meta.get("source", "?"))
        preview = doc.page_content[:150].replace("\n", " ")
        
        print(f"\n  [{i}] ì—°ë„:{year} | ì¶œì²˜:{source}")
        print(f"      ë‚´ìš©: {preview}...")
    
    # 6. ì¸ë±ìŠ¤ í†µê³„
    print(f"\n[6] ì¸ë±ìŠ¤ í†µê³„:")
    dept_stats = analyze_department_coverage(vs)
    print(f"  ì´ ë¬¸ì„œ ìˆ˜: {vs.index.ntotal}")
    print(f"  í•™ê³¼ë³„ ë¬¸ì„œ ìˆ˜:")
    for dept, count in sorted(dept_stats.items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f"    {dept}: {count}ê°œ")


def extract_dept_from_content(content: str) -> str:
    """ë¬¸ì„œ ë‚´ìš©ì—ì„œ í•™ê³¼ëª… ì¶”ì¶œ"""
    depts = [
        "ì»´í“¨í„°ê³µí•™ê³¼", "ì „ìê³µí•™ê³¼", "í™”í•™ê³µí•™ê³¼", "ê¸°ê³„ê³µí•™ê³¼",
        "ì‚°ì—…ê²½ì˜ê³µí•™ê³¼", "ê±´ì¶•í•™ê³¼", "ì†Œí”„íŠ¸ì›¨ì–´ìœµí•©í•™ê³¼",
        "ì „ìì •ë³´ê³µí•™ë¶€", "ê³µê³¼ëŒ€í•™", "ì „ìì •ë³´ëŒ€í•™"
    ]
    for dept in depts:
        if dept in content:
            return dept
    return "-"


def analyze_department_coverage(vs: FAISS) -> dict:
    """ì¸ë±ìŠ¤ ë‚´ í•™ê³¼ë³„ ë¬¸ì„œ ì»¤ë²„ë¦¬ì§€ ë¶„ì„"""
    dept_counts = {}
    try:
        for doc_id, doc in vs.docstore._dict.items():
            dept = extract_dept_from_content(doc.page_content)
            if dept != "-":
                dept_counts[dept] = dept_counts.get(dept, 0) + 1
    except Exception:
        pass
    return dept_counts


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
    queries = [
        "ì»´í“¨í„°ê³µí•™ê³¼ ì¡¸ì—…ìš”ê±´ ì•Œë ¤ì¤˜",
        "ì „ìê³µí•™ê³¼ ì¡¸ì—…ìš”ê±´",
        "ì „ìì •ë³´ëŒ€í•™ ì¡¸ì—…í•™ì ",
    ]
    
    for query in queries:
        diagnose_search(query)
        print("\n\n")
