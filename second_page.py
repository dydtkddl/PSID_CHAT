# --- second_page.py (SPARQL ë¼ìš°íŒ… + RAG í´ë°±) ---
import os
import re
import mimetypes
import ntpath
import unicodedata
import uuid
from pathlib import Path
from typing import Optional, List, Dict, Tuple

import pandas as pd
import streamlit as st

# íŒŒì„œ/ë¼ìš°í„° (ìˆìœ¼ë©´ Lark, ì—†ìœ¼ë©´ ì •ê·œì‹ ë¼ìš°í„°)
try:
    from query_parser import parse_query
except Exception:
    from query_router import query_router as parse_query

from reranker import rerank

# LangChain ë¬¸ì„œ íƒ€ì… í˜¸í™˜
try:
    from langchain.schema import Document as LC_Document
except Exception:
    try:
        from langchain_core.documents import Document as LC_Document
    except Exception:
        LC_Document = None

# ë‚´ë¶€ ì²´ì¸ (FAISS RAG)
from chains import get_multi_year_vector_store, get_retreiver_chain, get_conversational_rag
from langchain_core.messages import HumanMessage, AIMessage
from langsmith import Client
from langchain_core.tracers.context import collect_runs

# KG(Fuseki) í´ë¼ì´ì–¸íŠ¸ â€“ ì´ë²ˆ ìˆ˜ì •ì˜ í•µì‹¬
from kg_client import (
    q_article15_details,
    q_article15_files_pages,
    q_article15_sameas,
    q_since_date,
    q_count_article_or_clause_none,
    q_undergrad_top5_for_cohort,
    require_rows,
    bindings_to_table,
    get_config,
)

client = Client()
APP_DIR = Path(__file__).resolve().parent

CATEGORIES = {
    "ê·œì •": "regulations",
    "í•™ë¶€ ì‹œí–‰ì„¸ì¹™": "undergrad_rules",
    "ëŒ€í•™ì› ì‹œí–‰ì„¸ì¹™": "grad_rules",
    "í•™ì‚¬ì œë„": "academic_system",
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# íŒŒì¼ ê²€ìƒ‰ìš©(ë‹¤ìš´ë¡œë“œ ë²„íŠ¼)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SEARCH_ROOTS_DEFAULT = [
    APP_DIR / "past_documents",
    APP_DIR / "todo_documents",
    APP_DIR / "docs",
    APP_DIR / "backup",
    Path.cwd() / "past_documents",
    Path.cwd() / "todo_documents",
    Path.cwd() / "docs",
    Path.cwd() / "backup",
]
SEARCH_EXTS = {".pdf", ".PDF"}

def _basename_crossplat(p: str) -> str:
    if not p:
        return ""
    p = p.strip().strip('"').strip("'")
    name = ntpath.basename(p)
    name = name.split("/")[-1].split("\\")[-1]
    return unicodedata.normalize("NFC", name)

def _strip_source_prefix(snippet: str, fname: str) -> str:
    if not snippet:
        return ""
    if fname:
        snippet = re.sub(rf"(?im)^\s*Source\s*:?\s*{re.escape(fname)}\s*", "", snippet)
    snippet = re.sub(r"(?im)^\s*Source\s*:\s*", "", snippet, count=1)
    return snippet.strip()

def _coerce_ctx_item(d) -> dict:
    """LangChain Document / dict / ë¬¸ìì—´ â†’ í™”ë©´ í‘œì¤€ ìŠ¤í‚¤ë§ˆë¡œ ì •ê·œí™”"""
    item = {"filename": "", "page": "", "url": "", "snippet": ""}

    def _basename(s: str) -> str:
        if not s:
            return ""
        s = s.strip().strip('"').strip("'")
        s = s.split("?", 1)[0].split("#", 1)[0]
        s = s.split("/")[-1].split("\\")[-1]
        return s

    # dict
    if isinstance(d, dict):
        meta = d.get("metadata") or {}
        text = (d.get("page_content") or d.get("content") or "") or ""
        fname = meta.get("filename") or _basename(meta.get("source", ""))
        page  = meta.get("page") or meta.get("page_number") or meta.get("pageIndex") or ""
        url   = meta.get("url") or meta.get("source_url") or meta.get("document_url") or ""
        if not fname and text:
            first = text.splitlines()[0].strip()
            if first.lower().startswith("source"):
                maybe = first.split(":", 1)[-1].strip()
                fname = _basename(maybe)
        text = _strip_source_prefix(text, fname)
        text = re.sub(r"\s+", " ", text).strip()
        if len(text) > 280:
            text = text[:279] + "â€¦"
        item.update({"filename": fname or "", "page": str(page) if page is not None else "", "url": url or "", "snippet": text})
        return item

    # LC Document
    if LC_Document is not None and isinstance(d, LC_Document):
        meta = getattr(d, "metadata", {}) or {}
        text = getattr(d, "page_content", "") or ""
        fname = meta.get("filename") or _basename(meta.get("source", ""))
        page  = meta.get("page") or meta.get("page_number") or meta.get("pageIndex") or ""
        url   = meta.get("url") or meta.get("source_url") or meta.get("document_url") or ""
        if not fname and text:
            first = text.splitlines()[0].strip()
            if first.lower().startswith("source"):
                maybe = first.split(":", 1)[-1].strip()
                fname = _basename(maybe)
        text = _strip_source_prefix(text, fname)
        text = re.sub(r"\s+", " ", text).strip()
        if len(text) > 280:
            text = text[:279] + "â€¦"
        item.update({"filename": fname or "", "page": str(page) if page is not None else "", "url": url or "", "snippet": text})
        return item

    # Fallback ë¬¸ìì—´
    s = str(d or "")
    m = re.search(r"page_content\s*=\s*['\"](.*?)['\"]\s*,", s, flags=re.S)
    text = m.group(1) if m else s
    fname = ""
    first = text.splitlines()[0].strip() if text else ""
    if first.lower().startswith("source"):
        maybe = first.split(":", 1)[-1].strip()
        fname = _basename(maybe)
    mpage = re.search(r"[{,]\s*['\"]?(page|page_number|pageIndex)['\"]?\s*:\s*['\"]?(\d+)['\"]?", s)
    page = mpage.group(2) if mpage else ""
    text = _strip_source_prefix(text, fname)
    text = re.sub(r"\s+", " ", text).strip()
    if len(text) > 280:
        text = text[:279] + "â€¦"
    item.update({"filename": fname or "", "page": str(page) if page is not None else "", "url": "", "snippet": text})
    return item

def _tokenize_name(s: str) -> List[str]:
    s = unicodedata.normalize("NFC", s or "")
    toks = re.findall(r"[0-9A-Za-zê°€-í£]+", s)
    return [t for t in (toks or []) if len(t) >= 2]

def _norm_key(s: str) -> str:
    return unicodedata.normalize("NFC", s or "").casefold().strip()

def _norm_key_noext(s: str) -> str:
    s = unicodedata.normalize("NFC", s or "").casefold().strip()
    s = re.sub(r"\.[a-z0-9]+$", "", s)
    s = re.sub(r"[\s_\-]+", "", s)
    s = re.sub(r"[(){}\[\]]", "", s)
    return s

@st.cache_resource(show_spinner=False)
def _build_source_index(extra_roots: Optional[List[Path]] = None) -> Dict[str, Dict]:
    roots: List[Path] = []
    seen = set()
    for r in (SEARCH_ROOTS_DEFAULT + (extra_roots or [])):
        try:
            rp = r.resolve()
            if rp.exists() and rp.is_dir() and str(rp) not in seen:
                roots.append(rp)
                seen.add(str(rp))
        except Exception:
            continue

    exact: Dict[str, str] = {}
    noext: Dict[str, List[str]] = {}
    tokens: Dict[str, set] = {}

    for root in roots:
        try:
            for p in root.rglob("*"):
                if p.is_file() and p.suffix in SEARCH_EXTS:
                    name = p.name
                    exact[_norm_key(name)] = str(p)
                    noext.setdefault(_norm_key_noext(name), []).append(str(p))
                    tokens[str(p)] = set(_tokenize_name(name))
        except Exception:
            continue

    return {"exact": exact, "noext": noext, "tokens": tokens}

def _find_source_file(filename: str) -> Optional[str]:
    if not filename:
        return None
    idx = _build_source_index()
    k = _norm_key(filename)
    if k in idx["exact"]:
        return idx["exact"][k]
    k2 = _norm_key_noext(filename)
    if k2 in idx["noext"]:
        cands = sorted(idx["noext"][k2], key=lambda x: len(x))
        return cands[0] if cands else None
    want = set(_tokenize_name(filename))
    best_path, best_score = None, 0
    if want:
        for path, toks in idx["tokens"].items():
            if not toks:
                continue
            score = len(want & toks)
            if score > best_score:
                best_score, best_path = score, path
    return best_path

def _overlap_score(a: str, b: str) -> float:
    ta = {t for t in re.findall(r"\w+", (a or "").lower()) if len(t) >= 2}
    tb = {t for t in re.findall(r"\w+", (b or "").lower()) if len(t) >= 2}
    if not ta or not tb:
        return 0.0
    inter = len(ta & tb)
    return inter / (len(tb) or 1)

def _strip_llm_source_lines(text: str) -> str:
    return re.sub(r"(?im)^\s*source\s*:\s*.*$", "", text).strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SPARQL ë¼ìš°íŒ… (ì—¬ê¸°ì„œ ë§¤ì¹­ë˜ë©´ FAISS RAGë¥¼ ê±´ë„ˆëœ€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _route_sparql(user_input: str) -> Optional[Tuple[str, List[List[str]], List[str]]]:
    """
    ë§¤ì¹­ë˜ë©´ (ì„¹ì…˜íƒ€ì´í‹€, í‘œë°ì´í„°, ì»¬ëŸ¼ëª…) ë°˜í™˜, ì•„ë‹ˆë©´ None
    """
    q = user_input.strip()

    # 1) ì œ15ì¡° â€¦ ì„¤ëª…
    if re.search(r"ì œ?15\s*ì¡°.*ì„¤ëª…", q):
        rows = q_article15_details(category="regulations")
        require_rows(rows, "ì œ15ì¡° ê´€ë ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        cols = ["s", "article", "clause", "label", "src", "page", "effFrom"]
        table = bindings_to_table(rows, cols)
        return ("ì œ15ì¡° ìƒì„¸", table, cols)

    # 2) 2025í•™ë²ˆ ê¸°ì¤€ â€¦ 2025-04-30 ì´í›„ íš¨ë ¥ â€¦ regulations
    if re.search(r"2025\s*í•™ë²ˆ.*2025-04-30.*(ì´í›„|ì´ìƒ).*íš¨ë ¥.*regulations", q, flags=re.I):
        rows = q_since_date("regulations", "2025", "2025-04-30")
        require_rows(rows, "í•´ë‹¹ ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        cols = ["s", "article", "clause", "effFrom", "src", "page"]
        table = bindings_to_table(rows, cols)
        return ("2025í•™ë²ˆ ê¸°ì¤€ 2025-04-30 ì´í›„ íš¨ë ¥ ì¡°í•­", table, cols)

    # 3) ì œ15ì¡°ë¡œ í‘œê¸°ëœ â€¦ íŒŒì¼/í˜ì´ì§€
    if re.search(r"ì œ?15\s*ì¡°.*(íŒŒì¼|í˜ì´ì§€)", q):
        rows = q_article15_files_pages("regulations")
        require_rows(rows, "ì œ15ì¡°ì˜ íŒŒì¼/í˜ì´ì§€ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        cols = ["src", "page"]
        table = bindings_to_table(rows, cols)
        return ("ì œ15ì¡° íŒŒì¼/í˜ì´ì§€", table, cols)

    # 4) ì œ15ì¡° URN â€¦ ë§¤í•‘ëœ Clause
    if re.search(r"ì œ?15\s*ì¡°.*URN.*ë§¤í•‘.*Clause", q, flags=re.I):
        rows = q_article15_sameas("regulations")
        require_rows(rows, "ì œ15ì¡° URN ë§¤í•‘ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        cols = ["s", "urn"]
        table = bindings_to_table(rows, cols)
        return ("ì œ15ì¡° URN sameAs", table, cols)

    # 5) article ë˜ëŠ” clause ê°’ì´ None
    if re.search(r"article\s*ë˜ëŠ”\s*clause.*None.*(ê°œìˆ˜|ìˆ˜)", q, flags=re.I):
        rows = q_count_article_or_clause_none("regulations")
        require_rows(rows, "ì¹´ìš´íŠ¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        cols = ["n"]
        table = bindings_to_table(rows, cols)
        return ("article/clause None ê°œìˆ˜", table, cols)

    # 6) í•™ë¶€(UG) + 2025í•™ë²ˆ â€¦ 5ê°œ
    if re.search(r"(í•™ë¶€|UG).*(2025).*5\s*ê°œ", q, flags=re.I):
        rows = q_undergrad_top5_for_cohort("2025")
        require_rows(rows, "UG 2025 ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        cols = ["s", "article", "clause", "effFrom", "src"]
        table = bindings_to_table(rows, cols)
        return ("í•™ë¶€ 2025 TOP5", table, cols)

    return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Cohort í—¬í¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _list_available_cohorts(slug: str) -> List[str]:
    base = APP_DIR / "faiss_db" / slug
    out = []
    if base.exists():
        for p in base.iterdir():
            if p.is_dir() and (p / "index.faiss").exists():
                out.append(p.name)
    try:
        out.sort(key=lambda x: int(x), reverse=True)
    except Exception:
        out.sort(reverse=True)
    return out

def _infer_default_cohort(student_id: Optional[str], cohorts: List[str]) -> int:
    if not cohorts:
        return 0
    if not student_id:
        return 0
    digits = "".join(ch for ch in str(student_id) if ch.isdigit())
    candidates = []
    if len(digits) >= 4:
        candidates.append(digits[:4])
    if len(digits) >= 2:
        yy = int(digits[:2])
        if 0 <= yy <= 99:
            candidates.append(f"20{yy:02d}")
    for c in candidates:
        if c in cohorts:
            return cohorts.index(c)
    return 0

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def second_page():
    st.header("Kyung Hee University's Regulations Chatbot")

    # íŒŒì¼ ì¸ë±ìŠ¤ ìºì‹œ ì¤€ë¹„
    _build_source_index()

    # ì¹´í…Œê³ ë¦¬ ì„ íƒ
    st.subheader("ê²€ìƒ‰ ë²”ì£¼ ì„ íƒ")
    labels = list(CATEGORIES.keys())
    default_idx = 0
    sel_label = st.radio(
        "ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:",
        labels,
        index=st.session_state.get("kb_category_idx", default_idx),
        horizontal=True,
    )
    sel_slug = CATEGORIES[sel_label]
    st.session_state["kb_category_idx"] = labels.index(sel_label)
    st.session_state.setdefault("kb_category_slug", sel_slug)
    changed_category = (st.session_state["kb_category_slug"] != sel_slug)
    st.session_state["kb_category_slug"] = sel_slug

    # ì½”í˜¸íŠ¸ ì„ íƒ(í•™ë¶€/ëŒ€í•™ì› ì‹œí–‰ì„¸ì¹™)
    st.session_state.setdefault("kb_cohort", {})
    cohort = None
    cohort_changed = False
    if sel_slug in ("undergrad_rules", "grad_rules"):
        cohorts = _list_available_cohorts(sel_slug)
        if not cohorts:
            st.error(
                "í•´ë‹¹ ë²”ì£¼ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì…í•™ë…„ë„ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
                f"ì˜ˆ: todo_documents/{sel_slug}/2020/ ì— ë¬¸ì„œë¥¼ ë„£ê³  "
                f"`python add_document.py --category {sel_slug} --cohort 2020` ì‹¤í–‰ í›„ ì´ìš©í•˜ì„¸ìš”."
            )
            return
        prev = st.session_state["kb_cohort"].get(sel_slug)
        default_idx = (
            _infer_default_cohort(st.session_state.get("student_id"), cohorts)
            if prev is None else (cohorts.index(prev) if prev in cohorts else 0)
        )
        sel_cohort = st.selectbox("ì…í•™ë…„ë„(í•™ë²ˆ) ì„ íƒ", cohorts, index=default_idx, key=f"cohort_{sel_slug}")
        cohort = sel_cohort
        cohort_changed = (prev != cohort)
        st.session_state["kb_cohort"][sel_slug] = cohort

    vs_key = f"{sel_slug}:{cohort or 'all'}"

    # ìƒë‹¨ ë²„íŠ¼
    col1, col2 = st.columns([1, 1])
    with col1:
        if st.button("Go to Home", key="home_page"):
            for k in ["student_id", "chat_histories", "vector_stores", "dialog_identifier", "kb_cohort"]:
                st.session_state.pop(k, None)
            st.rerun()
    with col2:
        if st.button("Refresh", key="refresh"):
            if "chat_histories" in st.session_state:
                st.session_state["chat_histories"][vs_key] = []
            st.session_state.pop("dialog_identifier", None)
            st.rerun()

    # ì„¸ì…˜ ìƒíƒœ
    st.session_state.setdefault("dialog_identifier", uuid.uuid4())
    st.session_state.setdefault("vector_stores", {})
    st.session_state.setdefault("chat_histories", {})
    st.session_state["chat_histories"].setdefault(vs_key, [])

    # ë²¡í„°ìŠ¤í† ì–´ ì¤€ë¹„ (RAG í´ë°±ìš©)
    vs = st.session_state["vector_stores"].get(vs_key)
    if (vs is None) or changed_category or cohort_changed:
        try:
            vs = get_multi_year_vector_store(sel_slug, primary_cohort=cohort)
            st.session_state["vector_stores"][vs_key] = vs
        except FileNotFoundError:
            if sel_slug in ("undergrad_rules", "grad_rules"):
                st.error(
                    f"ì„ íƒí•œ ë²”ì£¼/ì—°ë„('{sel_label} / {cohort}')ì— ëŒ€í•œ ë²¡í„° DBê°€ ì—†ìŠµë‹ˆë‹¤.\n"
                    f"todo_documents/{sel_slug}/{cohort}/ ì— ë¬¸ì„œë¥¼ ë„£ê³ \n"
                    f"`python add_document.py --category {sel_slug} --cohort {cohort}`ë¡œ ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•´ ì£¼ì„¸ìš”."
                )
            else:
                st.error(f"ì„ íƒí•œ ë²”ì£¼('{sel_label}')ì— ëŒ€í•œ ë²¡í„° DBê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € add_document.pyë¡œ êµ¬ì¶•í•´ ì£¼ì„¸ìš”.")
            return

    # ì´ì „ ëŒ€í™” ë Œë”ë§
    for message in st.session_state["chat_histories"][vs_key]:
        role = "AI" if isinstance(message, AIMessage) else "Human"
        with st.chat_message("AI" if role == "AI" else "Human"):
            st.write(message.content)

    # ì‚¬ìš©ì ì…ë ¥
    if user_input := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 'ì œ15ì¡° URNê³¼ ë§¤í•‘ëœ Clause ë³´ì—¬ì¤˜')"):
        st.chat_message("Human").write(user_input)

        # 0) ë¨¼ì € SPARQL ë¼ìš°íŒ… ì‹œë„
        try:
            routed = _route_sparql(user_input)
        except Exception as e:
            routed = None
            st.warning(f"SPARQL ë¼ìš°íŒ… ì˜¤ë¥˜: {e}")

        if routed:
            section, table, cols = routed
            # ê²°ê³¼ê°€ ë¹„ì–´ ìˆìœ¼ë©´ ì‹¤íŒ¨ ì²˜ë¦¬
            if not table:
                ai_text = "í•´ë‹¹ ì¡°ê±´ì˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
                st.chat_message("AI").write(ai_text)
                st.session_state["chat_histories"][vs_key].append(HumanMessage(content=user_input))
                st.session_state["chat_histories"][vs_key].append(AIMessage(content=ai_text))
            else:
                st.chat_message("AI").markdown(f"**{section}** â€” ì´ {len(table)}ê±´")
                df = pd.DataFrame(table, columns=cols)
                st.dataframe(df, use_container_width=True)
                # ì±„íŒ… íˆìŠ¤í† ë¦¬ ì €ì¥(ê°„ë‹¨ ìš”ì•½)
                ai_text = f"{section} â€” {len(table)}ê±´"
                st.session_state["chat_histories"][vs_key].append(HumanMessage(content=user_input))
                st.session_state["chat_histories"][vs_key].append(AIMessage(content=ai_text))
            return  # SPARQL ê²½ë¡œë©´ ì—¬ê¸°ì„œ ì¢…ë£Œ

        # 1) SPARQL ë§¤ì¹­ì´ ì•„ë‹ˆë©´ RAGë¡œ í´ë°±
        with collect_runs() as cb:
            with st.spinner("Searching..."):
                meta_filter, hints = parse_query(user_input)
                top_k = 7 if hints.get("wants_table") else 5
                history_retriever_chain = get_retreiver_chain(vs, meta_filter=meta_filter, top_k=top_k, primary_cohort=cohort)
                conversation_rag_chain = get_conversational_rag(history_retriever_chain)
                response = conversation_rag_chain.invoke(
                    {
                        "chat_history": st.session_state["chat_histories"][vs_key],
                        "input": user_input,
                        "student_id": st.session_state.get("student_id"),
                        "dialog_identifier": st.session_state["dialog_identifier"],
                    }
                )

                raw_answer = response.get("answer", "") or ""
                contexts = response.get("context", []) or []

                # ì»¨í…ìŠ¤íŠ¸ ì—†ìœ¼ë©´ ë°ì´í„°-ê¸°ë°˜ ì‘ë‹µ ê¸ˆì§€
                if not contexts:
                    ai_text = "í•´ë‹¹ ì¡°ê±´ì˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
                    st.chat_message("AI").write(ai_text)
                    st.session_state["chat_histories"][vs_key].append(HumanMessage(content=user_input))
                    st.session_state["chat_histories"][vs_key].append(AIMessage(content=ai_text))
                    return

                # ë¦¬ë­í‚¹
                try:
                    contexts = rerank(contexts or [], hints, user_input)
                except Exception:
                    pass

                answer = _strip_llm_source_lines(raw_answer)

                # ìƒìœ„ ì»¨í…ìŠ¤íŠ¸ ì„ ë³„
                TOPK_CONTEXTS = 5
                MIN_OVERLAP = 0.12
                normalized = [_coerce_ctx_item(d) for d in (contexts or [])]
                scored = []
                for c in normalized:
                    fname = (c.get("filename") or "").strip()
                    score = _overlap_score(answer, c.get("snippet", ""))
                    scored.append({**c, "_score": score, "_has_name": bool(fname)})
                filtered = [c for c in scored if c["_score"] >= MIN_OVERLAP]
                by_file = {}
                for c in filtered:
                    fname = (c.get("filename") or "").strip()
                    if not fname:
                        continue
                    best = by_file.get(fname)
                    if (best is None) or (c["_score"] > best["_score"]):
                        by_file[fname] = c
                coerced = sorted(by_file.values(), key=lambda x: x["_score"], reverse=True)[:TOPK_CONTEXTS]

                # Source ë¼ì¸
                source_files = [c["filename"] for c in coerced if c.get("filename")]
                if source_files:
                    answer = f"{answer}\n\nSource: " + ", ".join(source_files)

                st.chat_message("AI").write(answer)

                # ë¯¸ë¦¬ë³´ê¸°(ë‹¤ìš´ë¡œë“œ í¬í•¨)
                if coerced:
                    with st.expander("ğŸ“‘ ì°¸ê³ í•œ ë¬¸ì„œ ì¡°ê° (ë¯¸ë¦¬ë³´ê¸°)"):
                        for i, c in enumerate(coerced, 1):
                            header = c["filename"] or "ë¬¸ì„œ"
                            if c["page"]:
                                header += f" (p.{c['page']})"
                            st.markdown(f"**{i}. {header}**")
                            st.markdown(f"> {c['snippet']}")
                            bcol1, bcol2 = st.columns([1, 1], vertical_alignment="center")
                            with bcol1:
                                st.caption(" ")
                            with bcol2:
                                fname = c["filename"]
                                if fname:
                                    found_path = _find_source_file(fname)
                                    if found_path and os.path.exists(found_path):
                                        mime, _ = mimetypes.guess_type(fname)
                                        dl_key = f"ctxdl_{st.session_state.get('dialog_identifier','')}_{i}_{fname}"
                                        with open(found_path, "rb") as f:
                                            st.download_button(
                                                label=f"ğŸ“¥ {fname}",
                                                data=f,
                                                file_name=fname,
                                                mime=mime or "application/pdf",
                                                key=dl_key,
                                                use_container_width=True,
                                            )
                                else:
                                    st.caption(" ")

                # íˆìŠ¤í† ë¦¬ ì €ì¥
                st.session_state["chat_histories"][vs_key].append(HumanMessage(content=user_input))
                st.session_state["chat_histories"][vs_key].append(AIMessage(content=answer))

            st.session_state.run_id = cb.traced_runs[0].id if cb.traced_runs else None

    # (ì„ íƒ) í”¼ë“œë°± ìœ„ì ¯ ë“±ì€ í•„ìš” ì‹œ ìœ ì§€/ì‚­ì œ
