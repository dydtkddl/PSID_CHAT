# chains.py - RAG Pipeline for KHU Regulation Assistant
# Updated for langchain 1.2.x API

from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, Any, List

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.messages import BaseMessage

# RAG ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
SYSTEM_PROMPT = (
    f"ì˜¤ëŠ˜ ë‚ ì§œ: {datetime.now().strftime('%Y-%m-%d')}\n"
    "ë‹¹ì‹ ì€ ê²½í¬ëŒ€í•™êµ ê·œì • ì „ë¬¸ ê°€ìƒ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n\n"
    
    "## í•µì‹¬ ì—­í• \n"
    "- ì œê³µëœ ë¬¸ì„œ(Context)ë¥¼ **ê¼¼ê¼¼íˆ ì½ê³ ** ì‚¬ìš©ì ì§ˆë¬¸ì— **êµ¬ì²´ì ìœ¼ë¡œ** ë‹µë³€í•˜ì„¸ìš”.\n"
    "- ë¬¸ì„œì— ìˆëŠ” ì¡°í•­, ê¸ˆì•¡, ì¡°ê±´, ì ˆì°¨ ë“±ì˜ **ì„¸ë¶€ ì •ë³´**ë¥¼ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.\n"
    "- ë‹¨ìˆœíˆ '~í•©ë‹ˆë‹¤'ë¡œ ëë‚´ì§€ ë§ê³ , **ì™œ/ì–´ë–»ê²Œ/ì–¼ë§ˆë‚˜**ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.\n\n"
    
    "## ìš°ì„ ìˆœìœ„ ê·œì¹™\n"
    "1) ì—¬ëŸ¬ ë²„ì „ì´ ìˆìœ¼ë©´ **ê°€ì¥ ìµœì‹ ** versionDateë¥¼ ìš°ì„ í•©ë‹ˆë‹¤.\n"
    "2) ì‚¬ìš©ìì˜ ì˜ë„(í•™ë¶€/ëŒ€í•™ì›, ì…í•™ë…„ë„, ì¡°í•­)ì— ë§ëŠ” ë¬¸ì„œë¥¼ ìš°ì„ í•©ë‹ˆë‹¤.\n"
    "3) effectiveFrom/effectiveUntilì´ ì¶©ëŒí•˜ë©´ ëª…ì‹œì ìœ¼ë¡œ ì•Œë ¤ì£¼ì„¸ìš”.\n"
    "4) ì§ˆë¬¸í•œ í•™ê³¼ì˜ ì •ë³´ê°€ ë¬¸ì„œì— ì—†ë”ë¼ë„, ê´€ë ¨ ìˆëŠ” **ê³µí†µ ê·œì •**(ì¡¸ì—…í•™ì , êµì–‘ ìš”ê±´ ë“±)ì´ ìˆìœ¼ë©´ í•´ë‹¹ ë‚´ìš©ì„ ë‹µë³€í•˜ì„¸ìš”.\n"
    "   ë‹¨, 'í•´ë‹¹ í•™ê³¼ì˜ ê°œë³„ ì‹œí–‰ì„¸ì¹™ì€ í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ì— í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤'ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.\n"
    "5) ë¬¸ì„œì—ì„œ ì•„ë¬´ëŸ° ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ 'í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. í•™ê³¼ ì‚¬ë¬´ì‹¤ì´ë‚˜ ê²½í¬ëŒ€í•™êµ í¬íƒˆ(https://portal.khu.ac.kr)ì—ì„œ í™•ì¸í•´ ì£¼ì„¸ìš”.'ë¼ê³  ì•ˆë‚´í•˜ì„¸ìš”.\n\n"
    
    "## ì¶œë ¥ ê·œì¹™\n"
    "- ì¡°í•­, URI, ë‚´ìš©ì„ **ì ˆëŒ€ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”**.\n"
    "- ëª¨ë¥´ëŠ” ê°’ì€ '-'ë¡œ í‘œì‹œí•˜ì„¸ìš”.\n"
    "- Source: <íŒŒì¼ëª…> ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì„¸ìš”.\n\n"
    
    "## ì œê³µëœ ê·œì • ë¬¸ì„œ:\n"
)

# êµ¬ì¡°í™” ì„¹ì…˜ í…œí”Œë¦¿
ANSWER_FORMAT = """
ğŸ“Œ **ìš”ì•½**
[í•µì‹¬ ê²°ë¡  2~3ë¬¸ì¥]

ğŸ“‹ **ìƒì„¸ ë‚´ìš©**
[êµ¬ì²´ì ì¸ ì¡°ê±´, ê¸ˆì•¡, ì ˆì°¨, ê¸°ì¤€ ë“±ì„ ë¶ˆë¦¿ í¬ì¸íŠ¸ë¡œ]

ğŸ“ **ê·¼ê±° ì¡°í•­**
[ì‹¤ì œ ì¡°ë¬¸ ë²ˆí˜¸ì™€ ì œëª© ê¸°ì¬, ì˜ˆ: ì œ5ì¡°(ì „ê³µì´ìˆ˜í•™ì ), ì œ10ì¡°(ì¡¸ì—…ë…¼ë¬¸)]
- ì¶œì²˜: [ë¬¸ì„œëª…]

âš ï¸ **ì°¸ê³ ì‚¬í•­** (í•´ë‹¹ ì‹œì—ë§Œ)
[ì˜ˆì™¸ ì¡°ê±´, ì£¼ì˜í•  ì ]
"""


# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ (chains.pyê°€ ìœ„ì¹˜í•œ ê³³)
PROJECT_ROOT = Path(__file__).resolve().parent


def get_vector_store(category_slug: str, cohort: Optional[str] = None) -> FAISS:
    """
    ì¹´í…Œê³ ë¦¬(+ì½”í˜¸íŠ¸)ë³„ FAISS ë¡œë“œ
    - ê·œì •/í•™ì‚¬ì œë„: cohort=None â†’ faiss_db/<category>/
    - í•™ë¶€/ëŒ€í•™ì› ì‹œí–‰ì„¸ì¹™: cohort='2020' ë“± â†’ faiss_db/<category>/<cohort>/
    
    Note: Windowsì—ì„œ í•œê¸€ ê²½ë¡œ ë¬¸ì œë¥¼ ìš°íšŒí•˜ê¸° ìœ„í•´ ì„ì‹œ ë””ë ‰í† ë¦¬ì— ë³µì‚¬ í›„ ë¡œë“œí•©ë‹ˆë‹¤.
    cohort ê²½ë¡œê°€ ì—†ìœ¼ë©´ ì¹´í…Œê³ ë¦¬ ê¸°ë³¸ ì¸ë±ìŠ¤ë¡œ fallbackí•©ë‹ˆë‹¤.
    """
    import os
    import shutil
    import tempfile
    
    base = PROJECT_ROOT / "faiss_db" / category_slug
    
    # cohort ê²½ë¡œ ìš°ì„  ì‹œë„, ì—†ìœ¼ë©´ ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ë¡œ fallback
    if cohort:
        cohort_base = base / str(cohort)
        if (cohort_base / "index.faiss").exists():
            base = cohort_base
        # else: use category base (fallback)
    
    index_path = base / "index.faiss"
    pkl_path = base / "index.pkl"
    
    if not index_path.exists():
        raise FileNotFoundError(f"FAISS index not found for: {category_slug}")
    
    # í•œê¸€ ê²½ë¡œ ë¬¸ì œ ìš°íšŒ: ASCII ê²½ë¡œ ì„ì‹œ ë””ë ‰í† ë¦¬ì— ë³µì‚¬ í›„ ë¡œë“œ
    temp_dir = tempfile.mkdtemp(prefix="faiss_")
    try:
        # FAISS íŒŒì¼ë“¤ì„ ì„ì‹œ ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬
        shutil.copy2(str(index_path), os.path.join(temp_dir, "index.faiss"))
        if pkl_path.exists():
            shutil.copy2(str(pkl_path), os.path.join(temp_dir, "index.pkl"))
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ì—ì„œ ë¡œë“œ
        result = FAISS.load_local(
            temp_dir,
            embeddings=OpenAIEmbeddings(model="text-embedding-3-large"),
            allow_dangerous_deserialization=True,
        )
        return result
    finally:
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
        shutil.rmtree(temp_dir, ignore_errors=True)


def get_multi_year_vector_store(
    category_slug: str,
    primary_cohort: Optional[str] = None,
    max_fallback: int = 3,
) -> FAISS:
    """
    Cross-year fallback retrieval.
    primary_cohort (ì˜ˆ: "2025") ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•˜ê³ ,
    ì¸ì ‘ ì—°ë„ì˜ ì¸ë±ìŠ¤ë¥¼ mergeí•˜ì—¬ ê²€ìƒ‰ ë²”ìœ„ë¥¼ í™•ì¥í•©ë‹ˆë‹¤.

    ì „ëµ:
      1) primary_cohort ì¸ë±ìŠ¤ ë¡œë“œ
      2) ì¸ì ‘ ì—°ë„(ìµœì‹  â†’ ê³¼ê±°)ë¥¼ ìµœëŒ€ max_fallbackê°œê¹Œì§€ ë³‘í•©
      3) ê° ë¬¸ì„œ metadataì— '_cohort_year' íƒœê·¸ ì¶”ê°€

    cohortê°€ Noneì´ë©´ ì¹´í…Œê³ ë¦¬ í†µí•© ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not primary_cohort:
        return get_vector_store(category_slug, None)

    import re
    base_dir = PROJECT_ROOT / "faiss_db" / category_slug

    # ì‚¬ìš© ê°€ëŠ¥í•œ ì—°ë„ ëª©ë¡ ìŠ¤ìº”
    available_years = sorted(
        [
            d.name
            for d in base_dir.iterdir()
            if d.is_dir()
            and re.match(r"^\d{4}$", d.name)
            and (d / "index.faiss").exists()
        ],
        reverse=True,  # ìµœì‹ ìˆœ
    )

    if not available_years:
        # ì—°ë„ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ì¹´í…Œê³ ë¦¬ í†µí•© ì¸ë±ìŠ¤ ì‚¬ìš©
        return get_vector_store(category_slug, None)

    # primary ì—°ë„ ì¸ë±ìŠ¤ ë¡œë“œ
    primary_year = str(primary_cohort)
    try:
        merged_vs = get_vector_store(category_slug, primary_year)
        # primary ë¬¸ì„œì— ì—°ë„ íƒœê·¸ ì¶”ê°€
        _tag_cohort_year(merged_vs, primary_year)
    except FileNotFoundError:
        merged_vs = None

    # ì¸ì ‘ ì—°ë„ ì„ íƒ: primaryë³´ë‹¤ ê°€ê¹Œìš´ ì—°ë„ ìˆœì„œ (ìµœì‹  ìš°ì„ )
    fallback_years = [
        y for y in available_years
        if y != primary_year
    ]
    # ê°€ê¹Œìš´ ì—°ë„ ìˆœ ì •ë ¬ (primaryì™€ì˜ ì°¨ì´ ì ˆëŒ€ê°’ ê¸°ì¤€)
    try:
        primary_int = int(primary_year)
        fallback_years.sort(key=lambda y: abs(int(y) - primary_int))
    except ValueError:
        pass

    # ìµœëŒ€ max_fallbackê°œ ë³‘í•©
    merged_count = 0
    for year in fallback_years:
        if merged_count >= max_fallback:
            break
        try:
            fallback_vs = get_vector_store(category_slug, year)
            _tag_cohort_year(fallback_vs, year)
            if merged_vs is None:
                merged_vs = fallback_vs
            else:
                merged_vs.merge_from(fallback_vs)
            merged_count += 1
        except FileNotFoundError:
            continue

    if merged_vs is None:
        return get_vector_store(category_slug, None)

    return merged_vs


def _tag_cohort_year(vs: FAISS, year: str) -> None:
    """FAISS vector storeì˜ ëª¨ë“  ë¬¸ì„œ metadataì— _cohort_year íƒœê·¸ ì¶”ê°€."""
    try:
        for doc_id in vs.docstore._dict:
            doc = vs.docstore._dict[doc_id]
            if hasattr(doc, "metadata"):
                doc.metadata["_cohort_year"] = year
    except Exception:
        pass  # íƒœê·¸ ì‹¤íŒ¨í•´ë„ ê²€ìƒ‰ì—ëŠ” ì˜í–¥ ì—†ìŒ


def format_docs(docs: List) -> str:
    """Format retrieved documents into context string"""
    parts = []
    for doc in docs:
        src = doc.metadata.get("source", doc.metadata.get("filename", "ì•Œ ìˆ˜ ì—†ìŒ"))
        year_tag = doc.metadata.get("_cohort_year", "")
        year_prefix = f"[{year_tag}ë…„ë„] " if year_tag else ""
        parts.append(f"Source: {year_prefix}{src}\n{doc.page_content}")
    return "\n\n---\n\n".join(parts)


def get_retriever_chain(
    vector_store: FAISS,
    meta_filter: Optional[Dict[str, Any]] = None,
    top_k: int = 5,
    primary_cohort: Optional[str] = None,
):
    """
    Hybrid retriever: semantic search + keyword boosting.
    í•™ê³¼ëª… ë“± í•µì‹¬ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì„œë¥¼ ë°˜ë“œì‹œ ê²°ê³¼ì— í¬í•¨ì‹œí‚´.
    """
    import re as _re
    
    # ë” ë§ì´ ê²€ìƒ‰ í›„ í•„í„°ë§
    fetch_k = top_k * 5
    skw = {"k": fetch_k}
    if meta_filter:
        skw["filter"] = {k: v for k, v in meta_filter.items() if v not in (None, "", [])}
    
    base_retriever = vector_store.as_retriever(search_kwargs=skw)
    
    # ì•Œë ¤ì§„ í•™ê³¼ëª… íŒ¨í„´ (í‚¤ì›Œë“œ ì¶”ì¶œìš©)
    DEPT_PATTERNS = [
        "ì „ìê³µí•™ê³¼", "ì»´í“¨í„°ê³µí•™ê³¼", "ì»´í“¨í„°ê³µí•™ë¶€", "í™”í•™ê³µí•™ê³¼", "ê¸°ê³„ê³µí•™ê³¼",
        "ì‚°ì—…ê²½ì˜ê³µí•™ê³¼", "ì›ìë ¥ê³µí•™ê³¼", "ê±´ì¶•ê³µí•™ê³¼", "ê±´ì¶•í•™ê³¼",
        "ì‚¬íšŒê¸°ë°˜ì‹œìŠ¤í…œê³µí•™ê³¼", "í™˜ê²½í•™ë°í™˜ê²½ê³µí•™ê³¼", "ì‹ ì†Œì¬ê³µí•™ê³¼",
        "ì •ë³´ì „ìì‹ ì†Œì¬ê³µí•™ê³¼", "ì†Œí”„íŠ¸ì›¨ì–´ìœµí•©í•™ê³¼", "ì¸ê³µì§€ëŠ¥í•™ê³¼",
        "ìƒì²´ì˜ê³µí•™ê³¼", "ë°˜ë„ì²´ê³µí•™ê³¼", "ì „ìì •ë³´ê³µí•™ë¶€",
        "ì‘ìš©ìˆ˜í•™ê³¼", "ì‘ìš©ë¬¼ë¦¬í•™ê³¼", "ì‘ìš©í™”í•™ê³¼", "ìš°ì£¼ê³¼í•™ê³¼",
        "ì‹í’ˆìƒëª…ê³µí•™ê³¼", "ìœ ì „ìƒëª…ê³µí•™ê³¼", "ì›ì˜ˆìƒëª…ê³µí•™ê³¼",
        "í•œë°©ìƒëª…ê³µí•™ê³¼", "ìŠ¤ë§ˆíŠ¸íŒœê³¼í•™ê³¼",
        "ìœµí•©ë°”ì´ì˜¤", "êµ­ì œí•™ê³¼", "ì•„ì‹œì•„í•™ê³¼",
    ]
    
    # ì•½ì–´ ë§¤í•‘
    DEPT_ALIASES = {
        "ì „ìê³¼": "ì „ìê³µí•™ê³¼", "ì „ê³µê³¼": "ì „ìê³µí•™ê³¼", "ì „ì": "ì „ìê³µí•™ê³¼",
        "ì»´ê³µê³¼": "ì»´í“¨í„°ê³µí•™ê³¼", "ì»´ê³µ": "ì»´í“¨í„°ê³µí•™ê³¼", "ì»´í“¨í„°": "ì»´í“¨í„°ê³µí•™ê³¼",
        "í™”ê³µê³¼": "í™”í•™ê³µí•™ê³¼", "í™”ê³µ": "í™”í•™ê³µí•™ê³¼",
        "ê¸°ê³µê³¼": "ê¸°ê³„ê³µí•™ê³¼", "ê¸°ê³„": "ê¸°ê³„ê³µí•™ê³¼",
        "ì‚°ê³µê³¼": "ì‚°ì—…ê²½ì˜ê³µí•™ê³¼", "ì‚°ê³µ": "ì‚°ì—…ê²½ì˜ê³µí•™ê³¼",
        "ì›ìë ¥": "ì›ìë ¥ê³µí•™ê³¼",
        "ê±´ì¶•": "ê±´ì¶•ê³µí•™ê³¼",
        "í™˜ê²½": "í™˜ê²½í•™ë°í™˜ê²½ê³µí•™ê³¼",
        "ì†Œìœµ": "ì†Œí”„íŠ¸ì›¨ì–´ìœµí•©í•™ê³¼", "ì†Œí”„íŠ¸ì›¨ì–´": "ì†Œí”„íŠ¸ì›¨ì–´ìœµí•©í•™ê³¼",
        "ë°˜ë„ì²´": "ë°˜ë„ì²´ê³µí•™ê³¼",
        "ì¸ê³µì§€ëŠ¥": "ì¸ê³µì§€ëŠ¥í•™ê³¼", "AI": "ì¸ê³µì§€ëŠ¥í•™ê³¼",
        "ìƒì˜ê³µ": "ìƒì²´ì˜ê³µí•™ê³¼",
        "ì‹ ì†Œì¬": "ì‹ ì†Œì¬ê³µí•™ê³¼",
    }
    
    def _extract_dept_keywords(query: str) -> List[str]:
        """ì¿¼ë¦¬ì—ì„œ í•™ê³¼ëª… í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        # ì •ì‹ í•™ê³¼ëª… ë§¤ì¹­
        for dept in DEPT_PATTERNS:
            if dept in query:
                keywords.append(dept)
        # ì•½ì–´ ë§¤ì¹­
        for alias, full_name in DEPT_ALIASES.items():
            if alias in query and full_name not in keywords:
                keywords.append(full_name)
        return keywords
    
    def _keyword_search(keywords: List[str], max_results: int = 10) -> List:
        """FAISS docstoreì—ì„œ í‚¤ì›Œë“œ í¬í•¨ ë¬¸ì„œ ì§ì ‘ ê²€ìƒ‰"""
        results = []
        seen_ids = set()
        try:
            for doc_id, doc in vector_store.docstore._dict.items():
                if doc_id in seen_ids:
                    continue
                content = getattr(doc, "page_content", "")
                for kw in keywords:
                    if kw in content:
                        results.append(doc)
                        seen_ids.add(doc_id)
                        break
                if len(results) >= max_results:
                    break
        except Exception:
            pass
        return results
    
    class HybridRetriever:
        def __init__(self, retriever, final_k, target_year=None):
            self.retriever = retriever
            self.final_k = final_k
            self.target_year = target_year
        
        def _score_korean(self, content: str) -> float:
            if not content:
                return 0
            korean_chars = sum(1 for c in content if '\uac00' <= c <= '\ud7a3')
            total_chars = len(content.replace(" ", "").replace("\n", ""))
            return korean_chars / max(total_chars, 1)
        
        def _score_year(self, doc) -> float:
            if not self.target_year:
                return 0.5
            doc_year = doc.metadata.get("_cohort_year", "")
            if not doc_year:
                return 0.3
            try:
                diff = abs(int(doc_year) - int(self.target_year))
                return max(0, 1.0 - diff * 0.2)
            except (ValueError, TypeError):
                return 0.3
        
        def invoke(self, query: str) -> List:
            # 1) Semantic search
            semantic_docs = self.retriever.invoke(query)
            
            # 2) Keyword search for department names
            dept_keywords = _extract_dept_keywords(query)
            keyword_docs = _keyword_search(dept_keywords, max_results=10) if dept_keywords else []
            
            # 3) Merge: keyword docs first, then semantic (deduplicate)
            seen_content = set()
            all_docs = []
            
            # Add keyword-matched docs with boost flag
            for doc in keyword_docs:
                content_key = doc.page_content[:200]
                if content_key not in seen_content:
                    seen_content.add(content_key)
                    all_docs.append((doc, True))  # True = keyword match
            
            # Add semantic docs
            for doc in semantic_docs:
                content_key = doc.page_content[:200]
                if content_key not in seen_content:
                    seen_content.add(content_key)
                    all_docs.append((doc, False))
            
            # 4) Score and rank
            scored = []
            for i, (doc, is_keyword_match) in enumerate(all_docs):
                korean_score = self._score_korean(doc.page_content[:500])
                year_score = self._score_year(doc)
                rank_bonus = 1 - (i / max(len(all_docs), 1)) * 0.15
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ ë¬¸ì„œì— í° boost
                keyword_boost = 0.5 if is_keyword_match else 0.0
                
                # í•™ê³¼ëª…ì´ contentì— ì§ì ‘ í¬í•¨ë˜ëŠ”ì§€ ì¶”ê°€ í™•ì¸
                content_match = 0.0
                if dept_keywords:
                    for kw in dept_keywords:
                        if kw in doc.page_content:
                            content_match = 0.3
                            break
                
                final_score = (
                    korean_score * 0.25
                    + year_score * 0.20
                    + rank_bonus * 0.10
                    + keyword_boost
                    + content_match
                )
                scored.append((final_score, doc))
            
            scored.sort(key=lambda x: x[0], reverse=True)
            return [doc for _, doc in scored[:self.final_k]]
        
        def get_relevant_documents(self, query: str) -> List:
            return self.invoke(query)
    
    return HybridRetriever(base_retriever, top_k, primary_cohort)


# ì˜¤íƒ€ í˜¸í™˜
get_retreiver_chain = get_retriever_chain


def get_conversational_rag(retriever):
    """
    End-to-end Conversational RAG chain using modern LCEL
    """
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    
    # Query Rewriting Prompt for context-aware search
    query_rewrite_prompt = ChatPromptTemplate.from_messages([
        ("system", 
         "ë‹¹ì‹ ì€ ëŒ€í™” ë§¥ë½ì„ ì´í•´í•˜ì—¬ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì¬ì‘ì„±í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n\n"
         "## ì—­í• \n"
         "ì‚¬ìš©ìì˜ ìµœì‹  ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™”ë¥¼ ì°¸ì¡°í•˜ëŠ” ê²½ìš°, ëŒ€í™” ë§¥ë½ì„ íŒŒì•…í•˜ì—¬ "
         "**ë…ë¦½ì ì´ê³  ì™„ì „í•œ ê²€ìƒ‰ ì¿¼ë¦¬**ë¡œ ì¬ì‘ì„±í•˜ì„¸ìš”.\n\n"
         "## ì˜ˆì‹œ\n"
         "ëŒ€í™”: 'í™”í•™ê³µí•™ê³¼ ì¡¸ì—…ìš”ê±´ ì•Œë ¤ì¤˜' -> 'ë‚˜ëŠ” 4í•™ë…„ì¸ë° ë­˜ ë“¤ì–´ì•¼í•´?' -> 'ì•„ë‹ˆ ê³¼ëª©ë§ì•¼'\n"
         "ì¬ì‘ì„±: 'ê²½í¬ëŒ€í•™êµ í™”í•™ê³µí•™ê³¼ 4í•™ë…„ ì „ê³µ ê³¼ëª© ëª©ë¡'\n\n"
         "ëŒ€í™”: 'ì¥í•™ê¸ˆ ì‹ ì²­ ë°©ë²•' -> 'ì–¸ì œê¹Œì§€ì•¼?'\n"
         "ì¬ì‘ì„±: 'ì¥í•™ê¸ˆ ì‹ ì²­ ë§ˆê°ì¼ ê¸°í•œ'\n\n"
         "## ê·œì¹™\n"
         "1. ì§ˆë¬¸ì´ ì´ë¯¸ ì™„ì „í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜\n"
         "2. ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ í•™ê³¼, í•™ë…„, ì£¼ì œ ë“±ì„ í¬í•¨\n"
         "3. ê²€ìƒ‰ì— ìµœì í™”ëœ í‚¤ì›Œë“œ í˜•íƒœë¡œ ì‘ì„±\n"
         "4. í•œêµ­ì–´ë¡œ ì‘ì„±"
        ),
        MessagesPlaceholder(variable_name="chat_history", optional=True),
        ("user", "í˜„ì¬ ì§ˆë¬¸: {input}\n\nê²€ìƒ‰ì— ì‚¬ìš©í•  ì¬ì‘ì„±ëœ ì¿¼ë¦¬:")
    ])
    
    query_rewriter = query_rewrite_prompt | llm | StrOutputParser()
    
    # Build the chain using LCEL
    def retrieve_and_format(inputs: dict):
        """Retrieve documents and format them with context-aware query rewriting"""
        original_query = inputs.get("input", "")
        chat_history = inputs.get("chat_history", [])
        
        # Rewrite query if there's conversation history
        if chat_history and len(chat_history) > 0:
            try:
                rewritten_query = query_rewriter.invoke({
                    "input": original_query,
                    "chat_history": chat_history
                })
                search_query = rewritten_query.strip()
            except Exception:
                search_query = original_query
        else:
            search_query = original_query
        
        docs = retriever.invoke(search_query)
        return {
            **inputs,
            "context": format_docs(docs),
            "_retrieved_docs": docs  # Keep for later extraction
        }
    
    # RAG prompt for answer generation
    rag_prompt = ChatPromptTemplate.from_messages([
        ("system", 
         SYSTEM_PROMPT
         + "\n\n{context}\n\n"
         "ìœ„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— **ìƒì„¸í•˜ê²Œ** ë‹µë³€í•˜ì„¸ìš”.\n\n"
         "ë‹µë³€ ì‘ì„± ì‹œ ì£¼ì˜ì‚¬í•­:\n"
         "1. ë¬¸ì„œì— ìˆëŠ” **êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ì¡°ê±´, ì ˆì°¨**ë¥¼ í¬í•¨í•˜ì„¸ìš”.\n"
         "2. ë‹¨ìˆœíˆ '~í•©ë‹ˆë‹¤'ë¡œ ëë‚´ì§€ ë§ê³  **ì„¸ë¶€ ë‚´ìš©**ì„ ì„¤ëª…í•˜ì„¸ìš”.\n"
         "3. ì—¬ëŸ¬ ì¡°í•­ì´ ê´€ë ¨ë˜ë©´ ëª¨ë‘ ì–¸ê¸‰í•˜ì„¸ìš”.\n"
         "4. ë°˜ë“œì‹œ **í•œêµ­ì–´**ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n\n"
         "ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:\n"
         + ANSWER_FORMAT
        ),
        MessagesPlaceholder(variable_name="chat_history", optional=True),
        ("user", "{input}")
    ])
    
    # Chain that retrieves, formats, and generates
    chain = (
        RunnableLambda(retrieve_and_format)
        | RunnablePassthrough.assign(
            answer=rag_prompt | llm | StrOutputParser()
        )
    )
    
    # Wrap to return context docs as well
    def invoke_with_context(inputs: dict):
        result = chain.invoke(inputs)
        return {
            "answer": result.get("answer", ""),
            "context": result.get("_retrieved_docs", []),
            "input": inputs.get("input", "")
        }
    
    return RunnableLambda(invoke_with_context)
